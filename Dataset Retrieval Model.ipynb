{"cells":[{"cell_type":"markdown","source":["# Mount and module/library setup\n","\n","In this step, the shortcut setup in your Google Drive will be mounted to this Google Colab Notebook in order for access. It is important the steps were followed correctly in the user guide and file names are not changed.\n","\n","There will be a pop-up window in which Google Colab will request access to your Google Drive, this is normal and must be accepted to progress."],"metadata":{"id":"Mw-T3D2xuB17"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"B5Vpy_f7-V8n"},"outputs":[],"source":["#Import google collab drive usage, mount then enter directory for file access.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/adamcao-1906735-project\n","\n","!pip install -q tensorflow-ranking\n","!pip install -U tensorflow_text\n","!pip install -q tf-models-official==2.4.0\n","!pip install -U nltk\n","\n","import pathlib\n","\n","import tensorflow as tf\n","import tensorflow_ranking as tfr\n","import tensorflow_text as tf_text\n","from google.protobuf import text_format\n","import bz2\n","import json\n","import pandas as pd\n","import re\n","from official.modeling import tf_utils\n","from official import nlp\n","from official.nlp import bert\n","# Load the required submodules\n","import official.nlp.optimization\n","import official.nlp.bert.bert_models\n","import official.nlp.bert.configs\n","import official.nlp.bert.run_classifier\n","import official.nlp.bert.tokenization\n","import official.nlp.data.classifier_data_lib\n","import official.nlp.modeling.losses\n","import official.nlp.modeling.models\n","import official.nlp.modeling.networks\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n"]},{"cell_type":"markdown","source":["# Fine-Tuning BERT\n","\n","This is the script run with hyperparameter settings to fine tune bert. This utilises the Train and Validation ELWC .tfrecord files from the data pipeline.\n","\n","The main hyperparameters to be adjusted are:\n","\n","*   train_input_pattern - This should be adjusted to the path of the ELWC data to be used, this can differ depending on which feature selections you want to train the model on.\n","*   eval_input_pattern - This should be adjusted to the path of the ELWC data to be used, this can differ depending on which feature selections you want to validate the models on.\n","*   bert_max_seq_length - This is the sequence length to train BERT on, it should correspond to the same sequence length as the input data.\n","*   model_dir - This is the directory location that the model will be saved to.\n","*   list_size - This is the max number of datasets in each ranking problem that can be processed, 31 is the highest Colab Pro Plus allows before running out of memory. (It is recommended to lower this on normal Colab accounts.)\n","*   loss - Loss should be chosen between approx_ndcg_loss and softmax_loss, the recommended is approx_ndcg_loss\n","* num_train_steps - Recommend 100-500 for quick testing, for an idea of how long it takes: 5000 steps takes 2-4 hours.\n","* checkpoint_secs - This is the number of seconds before a checkpoint is saved, this should be adjusted according to the training steps.\n","\n","Parameters used to train **NDCGTitleTags31256Final**\n","\n","```\n","--train_input_pattern=FormattedData/TitleTagsData/256TrainELWC.tfrecord \\\n","--eval_input_pattern=FormattedData/TitleTagsData/256ValELWC.tfrecord \\\n","--bert_config_file=cased_L-12_H-768_A-12/bert_config.json \\\n","--bert_init_ckpt=cased_L-12_H-768_A-12/bert_model.ckpt \\\n","--bert_max_seq_length=256 \\\n","--model_dir=models/NDCGTitleTags31256Final \\\n","--list_size=31 \\\n","--loss=approx_ndcg_loss \\\n","--train_batch_size=1 \\\n","--eval_batch_size=1 \\\n","--learning_rate=1e-5 \\\n","--num_train_steps=25000 \\\n","--num_eval_steps=100 \\\n","--checkpoint_secs=900 \\\n","--num_checkpoints=1000\n","```\n","\n","Parameters used to train **NDCGTitle31256Final**\n","\n","```\n","--train_input_pattern=FormattedData/TitleData/256TrainELWC.tfrecord \\\n","--eval_input_pattern=FormattedData/TitleData/256ValELWC.tfrecord \\\n","--bert_config_file=cased_L-12_H-768_A-12/bert_config.json \\\n","--bert_init_ckpt=cased_L-12_H-768_A-12/bert_model.ckpt \\\n","--bert_max_seq_length=256 \\\n","--model_dir=models/NDCGTitle31256Final \\\n","--list_size=31 \\\n","--loss=approx_ndcg_loss \\\n","--train_batch_size=1 \\\n","--eval_batch_size=1 \\\n","--learning_rate=1e-5 \\\n","--num_train_steps=5000 \\\n","--num_eval_steps=100 \\\n","--checkpoint_secs=900 \\\n","--num_checkpoints=1000\n","```\n","\n","\n"],"metadata":{"id":"Gu8X-351uGMQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvJvEPMmiUns"},"outputs":[],"source":["!ls\n","\n","!python bertPython/tfrbert_example.py \\\n","   --train_input_pattern=FormattedData/TitleTagsData/256TrainELWC.tfrecord \\\n","   --eval_input_pattern=FormattedData/TitleTagsData/256ValELWC.tfrecord \\\n","   --bert_config_file=cased_L-12_H-768_A-12/bert_config.json \\\n","   --bert_init_ckpt=cased_L-12_H-768_A-12/bert_model.ckpt \\\n","   --bert_max_seq_length=256 \\\n","   --model_dir=models/SampleModel \\\n","   --list_size=16 \\\n","   --loss=approx_ndcg_loss \\\n","   --train_batch_size=1 \\\n","   --eval_batch_size=1 \\\n","   --learning_rate=1e-5 \\\n","   --num_train_steps=250 \\\n","   --num_eval_steps=100 \\\n","   --checkpoint_secs=300 \\\n","   --num_checkpoints=1000"]},{"cell_type":"markdown","source":["# Training Evaluation Using TensorBoard\n","\n","Once model training is complete, the training process can be evaluated by using Tensorboard and loading in the training data generated. This can be done by running the following script:\n","\n","You will also be able to see the training metrics of the top performing models by uncommenting the other models (Only 1 should be uncommented at a time)"],"metadata":{"id":"xkvPRFSzxWDl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"T20_7569KERu"},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir=\"/content/drive/MyDrive/adamcao-1906735-project/models/SampleModel/\"\n","#%tensorboard --logdir=\"/content/drive/MyDrive/adamcao-1906735-project/models/NDCGTitle31256Final/\"\n","#%tensorboard --logdir=\"/content/drive/MyDrive/adamcao-1906735-project/models/NDCGTitleTags31256Final/\""]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"Dataset Retrieval Model.ipynb","provenance":[{"file_id":"1vGF5A1zSppFNDEAC897J7N_R5aMF-C1e","timestamp":1646515379047}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}