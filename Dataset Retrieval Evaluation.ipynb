{"cells":[{"cell_type":"markdown","source":["# Mount and module/library setup\n","\n","In this step, the shortcut setup in your Google Drive will be mounted to this Google Colab Notebook in order for access. It is important the steps were followed correctly in the user guide and file names are not changed.\n","\n","There will be a pop-up window in which Google Colab will request access to your Google Drive, this is normal and must be accepted to progress."],"metadata":{"id":"R9Hw30JdUZnP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDx1j60rSLKS"},"outputs":[],"source":["#Import google collab drive usage, mount then enter directory for file access.\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/adamcao-1906735-project\n","\n","!pip install -q tensorflow-ranking tensorflow-serving-api\n","!pip install -U tensorflow_text\n","!pip install -q tf-models-official==2.4.0\n","!pip install -Uq grpcio==1.26.0\n","\n","!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n","curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n","!apt update\n","!apt-get install tensorflow-model-server\n","\n","import pathlib\n","import tensorflow as tf\n","import tensorflow_ranking as tfr\n","import tensorflow_text as tf_text\n","from tensorflow_serving.apis import input_pb2\n","from google.protobuf import text_format\n","import bz2\n","import json\n","import pandas as pd\n","import re\n","import os\n","from official.modeling import tf_utils\n","from official import nlp\n","from official.nlp import bert\n","# Load the required submodules\n","import official.nlp.optimization\n","import official.nlp.bert.bert_models\n","import official.nlp.bert.configs\n","import official.nlp.bert.run_classifier\n","import official.nlp.bert.tokenization\n","import official.nlp.data.classifier_data_lib\n","import official.nlp.modeling.losses\n","import official.nlp.modeling.models\n","import official.nlp.modeling.networks"]},{"cell_type":"markdown","source":["# Training Evaluation Using TensorBoard\n","\n","Once model training is complete, the training process can be evaluated by using Tensorboard and loading in the training data generated. This can be done by running the following script:"],"metadata":{"id":"8Y9QxyDsxy4F"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcf45e-TSjsl"},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir=\"/content/drive/MyDrive/adamcao-1906735-project/models/SampleModel/\"\n","#%tensorboard --logdir=\"/content/drive/MyDrive/adamcao-1906735-project/models/NDCGTitle31256Final/\"\n","#%tensorboard --logdir=\"/content/drive/MyDrive/adamcao-1906735-project/models/NDCGTitleTags31256Final/\""]},{"cell_type":"markdown","source":["# Start Tensorflow Serving Server\n","\n","This starts the Tensorflow Serving server using the model you wish use in order to make predictions, it will choose the best model by loss in the training iterations."],"metadata":{"id":"3-PPZGT9Ug6Z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-tFrMLk0_Hz"},"outputs":[],"source":["%%bash --bg\n","%cd /\n","tensorflow_model_server \\\n","  --port=8500 \\\n","  --rest_api_port=8501 \\\n","  --model_name=tfrbert \\\n","  --model_base_path=/content/drive/MyDrive/adamcao-1906735-project/models/SampleModel/export/best_model_by_loss >server.log 2>&1"]},{"cell_type":"markdown","source":["# Make and export predictions\n","\n","As the Tensorflow Serving Server is now running, this calls the prediction script using the Test data, formatted previously using the model chosen when starting the serving server. Sequence length should match the sequence length the model was trained on.\n","\n","It will export the predictions in the output folder inside the folder of the model.\n","\n","A successful example output:\n","\n","```\n"," * Running with arguments: Namespace(do_lower_case=False, input_file='FormattedData/testJSON.json', output_file='SampleModel/output/testscores.json', sequence_length=256, vocab_file='cased_L-12_H-768_A-12/vocab.txt')\n"," * Generating predictions for JSON ranking problems (filename: FormattedData/MainTest.json)\n","\n","Predicting 96 / 96 (100.00%)\n"," * exportRankingOutput(): Exporting scores to JSON (SampleModel/output/testscores.json)\n"," * Total execution time: 1:43:01.389\n","```\n","\n"],"metadata":{"id":"EifSMuguUySG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxCrqULy1PlU"},"outputs":[],"source":["outputFolder = 'models/SampleModel/output/'\n","if not os.path.exists(outputFolder):\n","    os.makedirs(outputFolder)\n","\n","!BERT_DIR=\"cased_L-12_H-768_A-12\"  && \\\n","sudo python bertPython/tfrbert_client_predict_from_json.py \\\n","    --vocab_file=${BERT_DIR}/vocab.txt \\\n","    --sequence_length=256 \\\n","    --input_file=FormattedData/MainTest.json \\\n","    --output_file=models/SampleModel/output/testscores.json"]},{"cell_type":"markdown","source":["# nDCG Scoring Evaluation of Models\n","\n","Using the functions below, it runs a nDCG scoring evaluation of each model in the /models folder and prints the nDCG scores for each one. It uses the predicted results from the exported testscores.json produced from the Serving server."],"metadata":{"id":"OC_qpWgLXFX3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vEX4UGecWesZ"},"outputs":[],"source":["from sklearn.metrics import ndcg_score, dcg_score\n","import numpy as np\n","\n","#https://finisky.github.io/2019/04/24/ndcg/\n","import math,os\n","\n","def NdcgFunc(rankedDataset, p, customGain = None):\n","    if len(rankedDataset) == 0:\n","        return 0\n","\n","    gain = customGain\n","    if gain == None:\n","        gain = [(pow(2, i) - 1) for i in range(p)]\n","\n","    ndcgSum = 0\n","    count = 0\n","    for singleList in rankedDataset:\n","        dcg = 0\n","        for i in range(min(len(singleList), p)):\n","            dcg += gain[singleList[i]] / math.log(i + 2)\n","\n","        idcg = 0\n","        optimalOrder = sorted(singleList, reverse = True)\n","        for i in range(min(len(optimalOrder), p)):\n","            if i >= p:\n","                break\n","            idcg += gain[optimalOrder[i]] / math.log(i + 2)\n","\n","        if abs(idcg) < 0.0001:\n","            continue\n","\n","        count += 1\n","        ndcgSum += dcg / idcg\n","\n","    return ndcgSum / count\n","\n","ndcg10Dict = {}\n","\n","def ndcgCalc(k, file):\n","  predictDataDir = \"models/\" + file + \"/output/\"\n","  g = open(predictDataDir + 'testscores.json')\n","  testDataPred = json.load(g)\n","  g.close()\n","\n","  predRelList = []\n","  for x in testDataPred['rankingProblemsOutput']:\n","    relList = []\n","    for z in x['documents']:\n","      relList.append(z['relevance'])\n","    predRelList.append(relList)\n","\n","  ndcgs = []\n","  ndcg10s = []\n","  for x in predRelList:\n","    # Relevance scores in output order\n","    relevance_scores = [x]\n","\n","    if len(x) > 1 and (1 in x or 2 in x):\n","      if k == -1:\n","        ndcgs.append( NdcgFunc(relevance_scores, len(x) ))\n","      else:\n","        ndcgs.append( NdcgFunc(relevance_scores,k) )\n","        if k == 10:\n","          ndcg10s.append(NdcgFunc(relevance_scores,k))\n","    else:\n","      ndcg10s.append(0.0)\n","\n","    ndcg10Dict[file] = ndcg10s\n","  return \"Avg - \" + str(round(np.average(ndcgs), 3 )) #+ \" Max:\" + str(np.max(ndcgs)) + \" Min:\" + str(np.min(ndcgs))\n","\n","#For all items use -1\n","def testeval(file):\n","  print(\"========================\")\n","  print(file)\n","  print(\"NDCGALL: \" + str(ndcgCalc(-1, file)))\n","  print(\"NDCG@03: \" + str(ndcgCalc(3, file)))\n","  print(\"NDCG@05: \" + str(ndcgCalc(5, file)))\n","  print(\"NDCG@10: \" + str(ndcgCalc(10, file)))\n","\n","root='models/'\n","dirlist = [ item for item in os.listdir(root) if os.path.isdir(os.path.join(root, item)) ]\n","\n","for x in dirlist:\n","  testeval(x)"]},{"cell_type":"markdown","source":["# nDCG@10 Histogram evaluation\n","\n","Here we can plot the histogram of the results produced by a specific model, showing all ranking problems and how it scored in each one."],"metadata":{"id":"93K_T32mhAwq"}},{"cell_type":"code","source":["# Import the libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#Number of topics in data\n","print(len(ndcg10Dict['SampleModel']))\n","# matplotlib histogram\n","plt.hist(np.asarray(ndcg10Dict['SampleModel'], dtype='float'), color = 'orange', edgecolor = 'black', bins = 96)\n","\n","# Add labels\n","plt.title('Histogram of nDCG@10 scores')\n","plt.xlabel('nDCG@10')\n","plt.ylabel('Number of queries')"],"metadata":{"id":"MzCewZ1lhY8c"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"background_execution":"on","collapsed_sections":[],"name":"Copy of Dataset Retrieval Evaluation.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}